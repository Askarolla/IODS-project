# Clustering and classification

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(MASS)
library(tidyverse)
library(ggplot2)
library(GGally)
library(corrplot)
data("Boston")

dim(Boston)
str(Boston)
summary(Boston)
```

There are 506 observations of 14 variables in the dataset. They are all either numerical or integer - in 2 cases. The variables contain data about housing in suburbs of Boston. There are measures of such as crime rates, nitrogen oxides concentration, population status, its location in relation to other city sites etc.

```{r}
# overview of variablesÂ´ distributions
ggpairs(Boston, 
        mapping = aes(alpha = 0.6), 
        lower = list(combo = wrap("facethist", bins = 30)))
```

Distribution of almost all variables displays strange trends. Only variable __rm__ or average number of rooms seems to follow normal distribution. Variables __crim__, __zn__, __chas__ and __blac__ are heavily scewed to one side. The case of __zn__ makes sense though because it is a dummy variable.

```{r}
# I will make a correlation matrix to look more closely into relationships between variables
cor_matrix<-cor(Boston) 
cor_matrix %>%
  round(digits = 2)
corrplot(cor_matrix, method="circle", type = "upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)
pairs(Boston)
```

When we look at relationships among variables, we see that the strongest negative correlation is between __dis__ (weighted mean of distances to five Boston employment centres) and __indus__ (proportion of non-retail business acres per town), __nox__ (nitrogen oxides concentration (parts per 10 million)) and __age__. These all logicaly make sense. __indus__ displays a strong positive correlation with __tax__ and __nox__. __chas__ has almost no correlation with the other because, again, it is a dummy.

```{r}
# scaling and turning into dataframe
scaled_boston <- scale(Boston)
summary(scaled_boston)
scaled_boston <- as.data.frame(scaled_boston)
```
In the scaled dataframe, mean is always zero. We can better asses the distribution of each variable from the scaled dataframe, the non-scaled numbers are hard to interpret without visualization

```{r}
# create a quantile vector of crim, create a categorical variable 'crime', remove original from dataset
bins <- quantile(scaled_boston$crim)
crime <- cut(scaled_boston$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))
scaled_boston <- dplyr::select(scaled_boston, -crim)
scaled_boston <- data.frame(scaled_boston, crime)

#take sample, create test & train sets
ind <- sample(nrow(scaled_boston),  size = nrow(scaled_boston) * 0.8)
train <- scaled_boston[ind,]
test <- scaled_boston[-ind,]
```

```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

classes <- as.numeric(train$crime)
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)

```

```{r}
#remove crime from test
correct_classes <- test$crime
test <- dplyr::select(test, -crime)

#predict and cross tabulate the results
lda.pred <- predict(lda.fit, newdata = test)
table(correct = correct_classes, predicted = lda.pred$class)
```

The biggest problem was to correctly distinguish between low and medium low. The extreme high value was predicted in all but one cases correctly. Overall, the model correctly guessed 65/102 (~ 64%) casses.

```{r}
#loading Boston and scaling it again
data("Boston")
scaled_boston <- scale(Boston)
scaled_boston <- as.data.frame(scaled_boston)
#calculating the distances
distb <- dist(scaled_boston)
#figuring out the optimal number of clusters
twcss <- sapply(1:10, function(k){kmeans(Boston, k)$tot.withinss})
# visualize the results
qplot(x = 1:10, y = twcss, geom = 'line')
```

Two clusters seem to be optimal.

```{r}
km <-kmeans(scaled_boston, centers = 2)
pairs(scaled_boston, col = km$cluster)
```

